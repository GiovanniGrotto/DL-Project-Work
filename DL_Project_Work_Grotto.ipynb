{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c861ddf",
   "metadata": {
    "id": "ISlElHcOxHfe",
    "papermill": {
     "duration": 0.004648,
     "end_time": "2024-12-09T17:13:03.314872",
     "exception": false,
     "start_time": "2024-12-09T17:13:03.310224",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PPO to play Atari Pong\n",
    "\n",
    "### The goal of this project work is to implement a PPO algorithm able to learn to play the atari game Pong and reaching a level where it can consistently win, the implementation will try to follow the original implementation from Jhon Schulman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "282c85b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T17:13:03.324566Z",
     "iopub.status.busy": "2024-12-09T17:13:03.323778Z",
     "iopub.status.idle": "2024-12-09T17:13:12.449082Z",
     "shell.execute_reply": "2024-12-09T17:13:12.447961Z"
    },
    "id": "Y4ZaxKMDwiP1",
    "outputId": "df38c500-627b-4be0-c62d-c062943df1ce",
    "papermill": {
     "duration": 9.132517,
     "end_time": "2024-12-09T17:13:12.451305",
     "exception": false,
     "start_time": "2024-12-09T17:13:03.318788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /opt/conda/lib/python3.10/site-packages (0.29.0)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (1.26.4)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (3.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (4.12.2)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (0.0.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52d7dad6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T17:13:12.461402Z",
     "iopub.status.busy": "2024-12-09T17:13:12.461058Z",
     "iopub.status.idle": "2024-12-09T17:13:20.863016Z",
     "shell.execute_reply": "2024-12-09T17:13:20.861937Z"
    },
    "id": "xlJ_V-hAwrm_",
    "outputId": "9de90b68-8223-443a-939a-7b974d9b8024",
    "papermill": {
     "duration": 8.409254,
     "end_time": "2024-12-09T17:13:20.865176",
     "exception": false,
     "start_time": "2024-12-09T17:13:12.455922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ale-py\r\n",
      "  Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\r\n",
      "Requirement already satisfied: numpy>1.20 in /opt/conda/lib/python3.10/site-packages (from ale-py) (1.26.4)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from ale-py) (4.12.2)\r\n",
      "Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: ale-py\r\n",
      "Successfully installed ale-py-0.10.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ale-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4bd77bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T17:13:20.876431Z",
     "iopub.status.busy": "2024-12-09T17:13:20.876116Z",
     "iopub.status.idle": "2024-12-09T17:13:37.216704Z",
     "shell.execute_reply": "2024-12-09T17:13:37.215783Z"
    },
    "id": "u40HxvC3wtES",
    "papermill": {
     "duration": 16.347822,
     "end_time": "2024-12-09T17:13:37.218704",
     "exception": false,
     "start_time": "2024-12-09T17:13:20.870882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/gymnasium/envs/registration.py:596: UserWarning: \u001b[33mWARN: plugin: shimmy.registration:register_gymnasium_envs raised Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 594, in load_plugin_envs\n",
      "    fn()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/shimmy/registration.py\", line 304, in register_gymnasium_envs\n",
      "    _register_atari_envs()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/shimmy/registration.py\", line 205, in _register_atari_envs\n",
      "    import ale_py\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ale_py/__init__.py\", line 68, in <module>\n",
      "    register_v0_v4_envs()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ale_py/registration.py\", line 178, in register_v0_v4_envs\n",
      "    _register_rom_configs(legacy_games, obs_types, versions)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ale_py/registration.py\", line 63, in _register_rom_configs\n",
      "    gymnasium.register(\n",
      "AttributeError: partially initialized module 'gymnasium' has no attribute 'register' (most likely due to a circular import)\n",
      "\u001b[0m\n",
      "  logger.warn(f\"plugin: {plugin.value} raised {traceback.format_exc()}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import wandb\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "\n",
    "import ale_py\n",
    "\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19476a38",
   "metadata": {
    "id": "34zzeVs-xncU",
    "papermill": {
     "duration": 0.0045,
     "end_time": "2024-12-09T17:13:37.227961",
     "exception": false,
     "start_time": "2024-12-09T17:13:37.223461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Define the Actor-Critic architecture, the CNN backbone to process the input images is shared and each component have a MLP head that will generate the actions and the critic-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baed67f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T17:13:37.238267Z",
     "iopub.status.busy": "2024-12-09T17:13:37.237306Z",
     "iopub.status.idle": "2024-12-09T17:13:37.245625Z",
     "shell.execute_reply": "2024-12-09T17:13:37.244930Z"
    },
    "id": "VC07DMxzwulR",
    "papermill": {
     "duration": 0.014995,
     "end_time": "2024-12-09T17:13:37.247172",
     "exception": false,
     "start_time": "2024-12-09T17:13:37.232177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Agent, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            layer_init(nn.Conv2d(4, 32, 8, stride=4)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(32, 64, 4, stride=2)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(64, 64, 3, stride=1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            layer_init(nn.Linear(64 * 6 * 6, 512)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.actor = layer_init(nn.Linear(512, 2), std=0.01)\n",
    "        self.critic = layer_init(nn.Linear(512, 1), std=1)\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(self.network(x))\n",
    "\n",
    "    # Here we sample the action randomly from the predicted distribution to maintain exploration\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        hidden = self.network(x)\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits) # No need for softmax because we specify logits\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)\n",
    "\n",
    "    # Here we get the best action from the predicted distribution\n",
    "    def act(self, x):\n",
    "        hidden = self.network(x)\n",
    "        logits = self.actor(hidden)\n",
    "        return torch.argmax(logits).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ca0a88",
   "metadata": {
    "id": "deeG2VaizNvg",
    "papermill": {
     "duration": 0.004138,
     "end_time": "2024-12-09T17:13:37.255625",
     "exception": false,
     "start_time": "2024-12-09T17:13:37.251487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Define some basic utils function to process the image given by the env into b&n and crop it and dave gif from a list of states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf46389c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T17:13:37.265104Z",
     "iopub.status.busy": "2024-12-09T17:13:37.264838Z",
     "iopub.status.idle": "2024-12-09T17:13:37.270738Z",
     "shell.execute_reply": "2024-12-09T17:13:37.270130Z"
    },
    "id": "pBe-d00Qw189",
    "papermill": {
     "duration": 0.012531,
     "end_time": "2024-12-09T17:13:37.272286",
     "exception": false,
     "start_time": "2024-12-09T17:13:37.259755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_pong(state):\n",
    "    if len(state.shape) == 3:\n",
    "      state = state[35:195]\n",
    "      state = state[::2, ::2, 0]  # downsample by factor of 2\n",
    "      state[state == 144] = 0  # erase background (background type 1)\n",
    "      state[state == 109] = 0  # erase background (background type 2)\n",
    "      state[state != 0] = 1  # everything else (paddles, ball) just set to 1\n",
    "    else:\n",
    "      state = state[:, 35:195]\n",
    "      state = state[:, ::2, ::2, 0]  # downsample by factor of 2\n",
    "      state[state == 144] = 0  # erase background (background type 1)\n",
    "      state[state == 109] = 0  # erase background (background type 2)\n",
    "      state[state != 0] = 1  # everything else (paddles, ball) just set to 1\n",
    "\n",
    "    return state.astype(np.int16)\n",
    "\n",
    "\n",
    "def save_gif_from_np(images, path, duration=100):\n",
    "    pil_images = [Image.fromarray((img*255).astype(np.int8)) for img in images]\n",
    "\n",
    "    # Save as a GIF\n",
    "    pil_images[0].save(\n",
    "        path,\n",
    "        save_all=True,\n",
    "        append_images=pil_images[1:],  # Add remaining frames\n",
    "        duration=100,  # Duration between frames in milliseconds\n",
    "        loop=0  # Loop forever\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8920beb",
   "metadata": {
    "id": "vmSwWKzVzXon",
    "papermill": {
     "duration": 0.004243,
     "end_time": "2024-12-09T17:13:37.280859",
     "exception": false,
     "start_time": "2024-12-09T17:13:37.276616",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Define the hyperparameters that will bu used by the PPo algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a3f5b2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T17:13:37.290967Z",
     "iopub.status.busy": "2024-12-09T17:13:37.290455Z",
     "iopub.status.idle": "2024-12-09T17:13:37.295635Z",
     "shell.execute_reply": "2024-12-09T17:13:37.294817Z"
    },
    "id": "-7-_AFzCw9Kp",
    "papermill": {
     "duration": 0.011919,
     "end_time": "2024-12-09T17:13:37.297299",
     "exception": false,
     "start_time": "2024-12-09T17:13:37.285380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    wandb_project_name = \"ppo\"\n",
    "    seed = 42\n",
    "    num_envs = 8\n",
    "    gym_id = \"ALE/Pong-v5\"\n",
    "    learning_rate = 2.5e-4\n",
    "    num_steps = 512\n",
    "    num_minibatches = 4\n",
    "    total_timesteps = 5000000\n",
    "    batch_size = int(num_envs * num_steps)\n",
    "    minibatch_size = int(batch_size // num_minibatches)\n",
    "    anneal_lr = True\n",
    "    gamma = 0.99\n",
    "    gae_lambda = 0.95\n",
    "    update_epochs = 4\n",
    "    clip_coef = 0.1\n",
    "    ent_coef = 0.01\n",
    "    vf_coef = 0.5\n",
    "    max_grad_norm = 0.5\n",
    "    test_every = int(total_timesteps // batch_size // 20)\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c3369e",
   "metadata": {
    "id": "vvq8WE7y0SQC",
    "papermill": {
     "duration": 0.004173,
     "end_time": "2024-12-09T17:13:37.305898",
     "exception": false,
     "start_time": "2024-12-09T17:13:37.301725",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Define the actual PPO algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3a62941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T17:13:37.316163Z",
     "iopub.status.busy": "2024-12-09T17:13:37.315892Z",
     "iopub.status.idle": "2024-12-09T17:13:37.337886Z",
     "shell.execute_reply": "2024-12-09T17:13:37.337060Z"
    },
    "id": "0sYFI5y4w91X",
    "papermill": {
     "duration": 0.029373,
     "end_time": "2024-12-09T17:13:37.339572",
     "exception": false,
     "start_time": "2024-12-09T17:13:37.310199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Initialize the memory buffers\n",
    "    obs = torch.zeros((config.num_steps, config.num_envs) + (4, w, h)).to(device)\n",
    "    actions = torch.zeros((config.num_steps, config.num_envs) + envs.single_action_space.shape).to(device)\n",
    "    logprobs = torch.zeros((config.num_steps, config.num_envs)).to(device)\n",
    "    rewards = torch.zeros((config.num_steps, config.num_envs)).to(device)\n",
    "    dones = torch.zeros((config.num_steps, config.num_envs)).to(device)\n",
    "    values = torch.zeros((config.num_steps, config.num_envs)).to(device)\n",
    "    ep_rewards = torch.zeros(config.num_envs).to(device)\n",
    "    last_4_obs_buffer = torch.zeros(config.num_envs, 4, w, h).to(device)\n",
    "    terminated_rw = []\n",
    "\n",
    "    # Game initialization\n",
    "    global_step = 0\n",
    "    start_time = time.time()\n",
    "    next_obs = torch.Tensor(preprocess_pong(envs.reset()[0])).to(device).unsqueeze(1)  # reset return obs, info\n",
    "    last_4_obs_buffer = torch.cat((last_4_obs_buffer[:, 1:], next_obs), dim=1)\n",
    "    next_obs = last_4_obs_buffer\n",
    "    next_done = torch.zeros(config.num_envs).to(device)\n",
    "    num_updates = int(config.total_timesteps // config.batch_size)\n",
    "\n",
    "    _ = envs.step(torch.ones(config.num_envs, dtype=torch.long)) # FIRE operation to make the game start\n",
    "\n",
    "    print(f\"Number of policy iteration: {num_updates}\")\n",
    "\n",
    "    for update in range(1, num_updates + 1):\n",
    "        if config.anneal_lr:\n",
    "            frac = 1.0 - (update - 1.0) / num_updates\n",
    "            lrnow = frac * config.learning_rate\n",
    "            lrnow = max(lrnow, 2e-4)\n",
    "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "        # Rollout: we gather experience using the policy\n",
    "        for step in range(0, config.num_steps):\n",
    "            global_step += 1 * config.num_envs\n",
    "            obs[step] = next_obs\n",
    "            dones[step] = next_done\n",
    "\n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "                values[step] = value.flatten()\n",
    "\n",
    "            actions[step] = action\n",
    "            logprobs[step] = logprob\n",
    "\n",
    "            next_obs, reward, done, truncated, info = envs.step(action.cpu().numpy()+2)\n",
    "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "            ep_rewards += rewards[step]\n",
    "            next_obs = torch.Tensor(preprocess_pong(next_obs)).to(device).unsqueeze(1)\n",
    "            last_4_obs_buffer = torch.cat((last_4_obs_buffer[:, 1:], next_obs), dim=1)\n",
    "            next_obs = last_4_obs_buffer\n",
    "            next_done = torch.logical_or(torch.Tensor(done).to(device), torch.Tensor(truncated).to(device)).int()\n",
    "            terminated_rw.extend(ep_rewards[next_done.bool()].tolist())\n",
    "            ep_rewards *= (1-next_done)\n",
    "\n",
    "        # Calculate the advantages and bootstrap value if not done\n",
    "        with torch.no_grad():\n",
    "            next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "\n",
    "            advantages = torch.zeros_like(rewards).to(device)\n",
    "            lastgaelam = 0\n",
    "            for t in reversed(range(config.num_steps)):\n",
    "                if t == config.num_steps - 1:\n",
    "                    nextnonterminal = 1.0 - next_done\n",
    "                    nextvalues = next_value\n",
    "                else:\n",
    "                    nextnonterminal = 1.0 - dones[t + 1]\n",
    "                    nextvalues = values[t + 1]\n",
    "                delta = rewards[t] + config.gamma * nextvalues * nextnonterminal - values[t]\n",
    "                advantages[t] = lastgaelam = delta + config.gamma * config.gae_lambda * nextnonterminal * lastgaelam\n",
    "            returns = advantages + values\n",
    "\n",
    "        # Adjust shapes\n",
    "        b_obs = obs.reshape((-1,) + (4, w, h))\n",
    "        b_logprobs = logprobs.reshape(-1)\n",
    "        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        b_inds = np.arange(config.batch_size)\n",
    "        clipfracs = []\n",
    "\n",
    "        # Policy update using the experience just played\n",
    "        for epoch in range(config.update_epochs):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, config.batch_size, config.minibatch_size):\n",
    "                end = start + config.minibatch_size\n",
    "                mb_inds = b_inds[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
    "                logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [((ratio - 1.0).abs() > config.clip_coef).float().mean().item()]\n",
    "\n",
    "                mb_advantages = b_advantages[mb_inds]\n",
    "                # Normalize\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - config.clip_coef, 1 + config.clip_coef)\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                newvalue = newvalue.view(-1)\n",
    "                v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                    newvalue - b_values[mb_inds],\n",
    "                    -config.clip_coef,\n",
    "                    config.clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "                # Final loss\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - config.ent_coef * entropy_loss + v_loss * config.vf_coef\n",
    "\n",
    "                # Network optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), config.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "        # Print and log data\n",
    "        if update % config.test_every == 0:\n",
    "            print(f\"Test reward at update {update}: {test(update)}\")\n",
    "\n",
    "        if len(ep_rewards) > 10:\n",
    "            mean_rw = torch.mean(torch.tensor(terminated_rw[-10:]))\n",
    "        else:\n",
    "            mean_rw = torch.mean(torch.tensor(terminated_rw))\n",
    "\n",
    "        writer.add_scalar(\"rewards/mean_reward\", mean_rw, global_step)\n",
    "        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "\n",
    "    print(f\"Final reward at update {update}: {test()}\")\n",
    "    envs.close()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9611ee",
   "metadata": {
    "id": "0B5lho2M0Wqh",
    "papermill": {
     "duration": 0.004109,
     "end_time": "2024-12-09T17:13:37.348030",
     "exception": false,
     "start_time": "2024-12-09T17:13:37.343921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The test function is uesd to evaluate a given policy, it choses the best moves according to the network instead of sampling from the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e263f57a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T17:13:37.357697Z",
     "iopub.status.busy": "2024-12-09T17:13:37.357452Z",
     "iopub.status.idle": "2024-12-09T17:13:37.364078Z",
     "shell.execute_reply": "2024-12-09T17:13:37.363308Z"
    },
    "id": "GAJv68DCw_k4",
    "papermill": {
     "duration": 0.013323,
     "end_time": "2024-12-09T17:13:37.365627",
     "exception": false,
     "start_time": "2024-12-09T17:13:37.352304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(update=None):\n",
    "    # Create the environment\n",
    "    env = gym.make(config.gym_id)\n",
    "    last_4_states = torch.zeros(1, 4, w, h).to(device)\n",
    "    done = False\n",
    "    terminated = False\n",
    "    total_reward = 0\n",
    "    state_list = []\n",
    "\n",
    "    # Reset the environment to get the initial state\n",
    "    state, info = env.reset()\n",
    "\n",
    "    state = preprocess_pong(state)\n",
    "    state_list.append(state)\n",
    "    state = torch.tensor(state).to(device)\n",
    "    last_4_states = torch.cat((last_4_states[:, 1:], state.unsqueeze(0).unsqueeze(0)), dim=1)\n",
    "\n",
    "    _ = env.step(1) # We need to do a FIRE operation to make the game starts\n",
    "\n",
    "    while not (done or terminated):\n",
    "        # Let the agent decide the action\n",
    "        with torch.no_grad():\n",
    "            action = agent.act(last_4_states)\n",
    "\n",
    "        # Take the action in the environment\n",
    "        next_state, reward, done, terminated, info = env.step(action+2)\n",
    "        # Update the total reward\n",
    "        total_reward += reward\n",
    "        # Transition to the next state\n",
    "        next_state = preprocess_pong(next_state)\n",
    "        state_list.append(next_state)\n",
    "        next_state = torch.tensor(next_state).to(device)\n",
    "        last_4_states = torch.cat((last_4_states[:, 1:], next_state.unsqueeze(0).unsqueeze(0)), dim=1)\n",
    "\n",
    "    game_name = config.gym_id[4:-3]\n",
    "    save_gif_from_np(state_list, f\"{game_name}_{update}.gif\")\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a829d227",
   "metadata": {
    "id": "vhGMDjxc0il7",
    "papermill": {
     "duration": 0.005349,
     "end_time": "2024-12-09T17:13:37.375265",
     "exception": false,
     "start_time": "2024-12-09T17:13:37.369916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Initalize the network and optimizer that will be used in the traninig of the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4bc95e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T17:13:37.384795Z",
     "iopub.status.busy": "2024-12-09T17:13:37.384556Z",
     "iopub.status.idle": "2024-12-09T17:13:48.857556Z",
     "shell.execute_reply": "2024-12-09T17:13:48.856824Z"
    },
    "id": "5Z-8eBGWxCY3",
    "outputId": "499ee312-b40c-4345-ad0d-9ea4773c147f",
    "papermill": {
     "duration": 11.480026,
     "end_time": "2024-12-09T17:13:48.859617",
     "exception": false,
     "start_time": "2024-12-09T17:13:37.379591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgiovannijgrotto\u001b[0m (\u001b[33mgiovannijgrotto-universit-di-bologna\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20241209_171344-t65gkvt2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mPong\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/giovannijgrotto-universit-di-bologna/ppo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/giovannijgrotto-universit-di-bologna/ppo/runs/t65gkvt2\u001b[0m\n",
      "A.L.E: Arcade Learning Environment (version 0.10.1+unknown)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "wandb.login(key=\"a128cb8af0ead22256607ab843b3c7e4e4dd4c48\")\n",
    "\n",
    "run_name = config.gym_id[4:-3]\n",
    "wandb.init(\n",
    "    project=config.wandb_project_name,\n",
    "    sync_tensorboard=True,\n",
    "    config=vars(config),\n",
    "    name=run_name,\n",
    "    monitor_gym=True,\n",
    "    save_code=True,\n",
    ")\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(config).items()])),\n",
    ")\n",
    "\n",
    "random.seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "torch.manual_seed(config.seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "envs = gym.make_vec(config.gym_id, num_envs=config.num_envs, vectorization_mode=\"sync\")\n",
    "\n",
    "w, h = preprocess_pong(envs.reset()[0]).shape[1:]\n",
    "\n",
    "agent = Agent().to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=config.learning_rate, eps=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2036f16",
   "metadata": {
    "id": "CNsr_N6r0qeX",
    "papermill": {
     "duration": 0.004824,
     "end_time": "2024-12-09T17:13:48.870021",
     "exception": false,
     "start_time": "2024-12-09T17:13:48.865197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let PPO train and visualize the progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5710b7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T17:13:48.881347Z",
     "iopub.status.busy": "2024-12-09T17:13:48.880755Z",
     "iopub.status.idle": "2024-12-09T18:37:20.271062Z",
     "shell.execute_reply": "2024-12-09T18:37:20.270007Z"
    },
    "id": "2KXawSiNxGJf",
    "outputId": "fa493d0e-a74c-48ec-bd28-a0ea79d53a8d",
    "papermill": {
     "duration": 5011.397999,
     "end_time": "2024-12-09T18:37:20.272922",
     "exception": false,
     "start_time": "2024-12-09T17:13:48.874923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of policy iteration: 1220\n",
      "Test reward at update 61: -17.0\n",
      "Test reward at update 122: -11.0\n",
      "Test reward at update 183: 3.0\n",
      "Test reward at update 244: 15.0\n",
      "Test reward at update 305: 12.0\n",
      "Test reward at update 366: 3.0\n",
      "Test reward at update 427: 12.0\n",
      "Test reward at update 488: 18.0\n",
      "Test reward at update 549: 15.0\n",
      "Test reward at update 610: 20.0\n",
      "Test reward at update 671: 20.0\n",
      "Test reward at update 732: 17.0\n",
      "Test reward at update 793: 15.0\n",
      "Test reward at update 854: 20.0\n",
      "Test reward at update 915: 18.0\n",
      "Test reward at update 976: 20.0\n",
      "Test reward at update 1037: 17.0\n",
      "Test reward at update 1098: 21.0\n",
      "Test reward at update 1159: 16.0\n",
      "Test reward at update 1220: 19.0\n",
      "Final reward at update 1220: 21.0\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd4827c",
   "metadata": {
    "id": "EPr7yhGW7MWT",
    "papermill": {
     "duration": 0.005843,
     "end_time": "2024-12-09T18:37:20.285219",
     "exception": false,
     "start_time": "2024-12-09T18:37:20.279376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5062.504794,
   "end_time": "2024-12-09T18:37:23.440026",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-09T17:13:00.935232",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0145bedbb193417dbc8b5dd182b6b2f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "139f663985d047eaa770739123cbbf36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "578d65af880046eea6cf0c3e39e53386": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1b3b36dd5914314bd43502c5a65a4a0",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6d2caa9d64664bca8ea76a556995c703",
      "value": "0.312 MB of 0.312 MB uploaded\r"
     }
    },
    "6d2caa9d64664bca8ea76a556995c703": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "70268d72bb474dc3ab509a3a20f2def9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d12faa957564a719a4c948cf61c92d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_578d65af880046eea6cf0c3e39e53386",
       "IPY_MODEL_fd0929538c7b4ab386bd897ecf0c8797"
      ],
      "layout": "IPY_MODEL_0145bedbb193417dbc8b5dd182b6b2f1"
     }
    },
    "e1b3b36dd5914314bd43502c5a65a4a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd0929538c7b4ab386bd897ecf0c8797": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70268d72bb474dc3ab509a3a20f2def9",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_139f663985d047eaa770739123cbbf36",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
